#!/usr/bin/env bash

# when running load tests the key store should be loaded with
# a bunch of credentials so its data shape is representive of
# a real production scenario
#
# it takes a long time to load lots of creds into a CouchDB database
# so generating creds and loading them into CouchDB while running
# a load tests isn't desirable. instead, this script can be run
# outside of the load test to generate .couch files for CouchDB
# databases containing various numbers of credentails. armed
# with the .couch files it's quick to start a key store aginst
# the .couch file in effect almost instantly creating a key
# store with lots of credentials.

SCRIPT_DIR_NAME="$( cd "$( dirname "$0" )" && pwd )"
source $SCRIPT_DIR_NAME/../util.sh

#
# this script accepts one optional argument (--cfs) which
# is used to specify the number of sets of credentials which
# should be saved in each couch file
#
COUCH_FILE_SIZE=1000000

while [[ 0 -ne $# ]]
do
    KEY="$1"
    shift
    case $KEY in
        --cfs)
            COUCH_FILE_SIZE=${1:-}
            shift
            ;;
        *)
            echo "usage: `basename $0` [--cfs <couch file size>]"
            exit 1
            ;;
    esac
done

if [ $# != 0 ]; then
    echo "usage: `basename $0`"
    exit 1
fi

#
# some initialization before we start the meat of this script
#
DATA_DIRECTORY=$SCRIPT_DIR_NAME/temp_couchdb
rm -rf $DATA_DIRECTORY >& /dev/null

yar_init_deployment "$DATA_DIRECTORY"

#
# create a key store in an isolated container
#
echo "Creating Key Store"
if ! KEY_STORE=$(create_key_store "" false); then
    echo "Failed to create key store"
    exit 1
fi
echo "Key Store available @ '$KEY_STORE'"
echo "Key Store data saved in '$DATA_DIRECTORY'"

#
# remove all old databases
#
echo "Removing all old databases from '$SCRIPT_DIR_NAME'"
rm -f $SCRIPT_DIR_NAME/*.creds.couch >& /dev/null

#
# iterate over each of the json files containing previously
# generated credentials taking a copy of the key store after
# each millionth upload and save the copy to the same
# directory as this script
#
TOTAL_NUMBER_OF_CREDS=0
# for CREDS in $SCRIPT_DIR_NAME/*.json
# for CREDS in $SCRIPT_DIR_NAME/00[0-5][0-9]-*.json
for CREDS in $SCRIPT_DIR_NAME/000[0-9]-*.json
do
    echo "Uploading '$CREDS'"

    STATUS_CODE=$(curl \
        -s \
        -o /dev/null \
        --write-out '%{http_code}' \
        -X POST \
        -H "Content-Type: application/json; charset=utf8" \
        -d @$CREDS \
        http://$KEY_STORE/_bulk_docs)
    if [ $? -ne 0 ] || [ "$STATUS_CODE" != "201" ]; then
        echo "Failed to upload '$CREDS' to key store @ '$KEY_STORE'"
        exit 2
    fi

    NUMBER_OF_CREDS=$(echo $CREDS | sed s/\\/.*\\/[[:digit:]]*-0*// | sed s/.json$//)
    TOTAL_NUMBER_OF_CREDS=$(python -c "print $TOTAL_NUMBER_OF_CREDS + int($NUMBER_OF_CREDS)")
    if [ $(($TOTAL_NUMBER_OF_CREDS % $COUCH_FILE_SIZE)) == 0 ];then
        #
        # local.ini for CouchDB should have been configured with
        #
        # [couchdb]
        # delayed_commits = false
        #
        # but in case not, we'll issue the request below to force
        # to 'manually' force a flush to disk
        #
        STATUS_CODE=$(curl \
            -s \
            -o /dev/null \
            --write-out '%{http_code}' \
            -X POST \
            -H "Content-Type: application/json; charset=utf8" \
            http://$KEY_STORE/_ensure_full_commit)
        if [ $? -ne 0 ] || [ "$STATUS_CODE" != "201" ]; then
            echo "Flush to disk failed on key store '$KEY_STORE'"
            exit 2
        fi

        DEST=$SCRIPT_DIR_NAME/$TOTAL_NUMBER_OF_CREDS.creds.couch
        echo "Creating '$DEST'"
        cp $DATA_DIRECTORY/Key-Store/data/creds.couch $DEST
    fi

done

exit 0
